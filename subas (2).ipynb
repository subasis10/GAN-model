{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJeh6ojs86jP",
        "outputId": "e3bc280a-1ab9-44d3-c4c8-feaa85a35168"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "s6MZ-Z4r9M17",
        "outputId": "511c3113-2cea-407a-b90c-a3f0e62bc3d6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjBnWOcd9OeL"
      },
      "outputs": [],
      "source": [
        "\n",
        "from os import listdir\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from keras.preprocessing.image import img_to_array\n",
        "from keras.preprocessing.image import load_img\n",
        "from numpy import savez_compressed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EwoTfapF9XF9"
      },
      "outputs": [],
      "source": [
        "\n",
        "def load_images(path, size=(256,512)):\n",
        "\tsrc_list, tar_list = list(), list()\n",
        "\t# enumerate filenames in directory, assume all are images\n",
        "\tfor filename in listdir(path):\n",
        "\t\t# load and resize the image\n",
        "\t\tpixels = load_img(path + filename, target_size=size)\n",
        "\t\t# convert to numpy array\n",
        "\t\tpixels = img_to_array(pixels)\n",
        "\t\t# split into satellite and map\n",
        "\t\tsat_img, map_img = pixels[:, :256], pixels[:, 256:]\n",
        "\t\tsrc_list.append(sat_img)\n",
        "\t\ttar_list.append(map_img)\n",
        "\treturn [asarray(src_list), asarray(tar_list)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87OBDfFd9bEd",
        "outputId": "994b6dd8-3783-4029-9dfa-554ac146f347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded:  (1096, 256, 256, 3) (1096, 256, 256, 3)\n",
            "Saved dataset:  maps_256.npz\n"
          ]
        }
      ],
      "source": [
        " path = 'drive/My Drive/maps/train/'\n",
        " [src_images, tar_images] = load_images(path)\n",
        " print('Loaded: ', src_images.shape, tar_images.shape)\n",
        " filename = 'maps_256.npz'\n",
        " savez_compressed(filename, src_images, tar_images)\n",
        " print('Saved dataset: ', filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo_K8EHP9iBO"
      },
      "outputs": [],
      "source": [
        "from numpy import load\n",
        "from matplotlib import pyplot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJYlzFEE_OgT",
        "outputId": "ef07ecd2-2966-4b43-9fa2-ed0d7b0ce1c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded:  (1096, 256, 256, 3) (1096, 256, 256, 3)\n"
          ]
        }
      ],
      "source": [
        "data = load('maps_256.npz')\n",
        "src_images, tar_images = data['arr_0'], data['arr_1']\n",
        "print('Loaded: ', src_images.shape, tar_images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rghiObvj_QnY"
      },
      "outputs": [],
      "source": [
        "from keras.initializers import RandomNormal\n",
        "from keras.models import Model\n",
        "from keras.models import Input\n",
        "from keras.layers import Concatenate\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Conv2DTranspose\n",
        "from keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IYei5fr8_-wW"
      },
      "outputs": [],
      "source": [
        "def define_discriminator(image_shape):\n",
        "\t\n",
        "\tinit = RandomNormal(stddev=0.02)\n",
        "\tin_src_image = Input(shape=image_shape)\n",
        "\n",
        "\tin_target_image = Input(shape=image_shape)\n",
        "\n",
        "\tmerged = Concatenate()([in_src_image, in_target_image])\n",
        "\t# C64\n",
        "\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(merged)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C128\n",
        "\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C256\n",
        "\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# C512\n",
        "\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# second last output layer\n",
        "\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\td = BatchNormalization()(d)\n",
        "\td = LeakyReLU(alpha=0.2)(d)\n",
        "\t# patch output\n",
        "\td = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n",
        "\tpatch_out = Activation('sigmoid')(d)\n",
        "\n",
        "\tmodel = Model([in_src_image, in_target_image], patch_out)\n",
        " \n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=opt, loss_weights=[0.5])\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l02KUZuIAboC"
      },
      "outputs": [],
      "source": [
        "def define_encoder_block(layer_in, n_filters, batchnorm=True):\n",
        "\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  g = Conv2D(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "  if batchnorm:\n",
        "    g = BatchNormalization()(g, training=True)\n",
        "  g = LeakyReLU(alpha=0.2)(g)\n",
        "  return g\n",
        "\n",
        "def decoder_block(layer_in, skip_in, n_filters, dropout=True):\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  g = Conv2DTranspose(n_filters, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(layer_in)\n",
        "  g = BatchNormalization()(g, training=True)\n",
        "  if dropout:\n",
        "    g = Dropout(0.5)(g, training=True)\n",
        "  g = Concatenate()([g, skip_in])\n",
        "  g = Activation('relu')(g)\n",
        "  return g\n",
        "\n",
        "def define_generator(image_shape=(256,256,3)):\n",
        "  init = RandomNormal(stddev=0.02)\n",
        "  in_image = Input(shape=image_shape)\n",
        "  e1 = define_encoder_block(in_image, 64, batchnorm=False)\n",
        "  e2 = define_encoder_block(e1, 128)\n",
        "  e3 = define_encoder_block(e2, 256)\n",
        "  e4 = define_encoder_block(e3, 512)\n",
        "  e5 = define_encoder_block(e4, 512)\n",
        "  e6 = define_encoder_block(e5, 512)\n",
        "  e7 = define_encoder_block(e6, 512)\n",
        "  b = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(e7)\n",
        "  b = Activation('relu')(b)\n",
        "  d1 = decoder_block(b, e7, 512)\n",
        "  d2 = decoder_block(d1, e6, 512)\n",
        "  d3 = decoder_block(d2, e5, 512)\n",
        "  d4 = decoder_block(d3, e4, 512, dropout=False)\n",
        "  d5 = decoder_block(d4, e3, 256, dropout=False)\n",
        "  d6 = decoder_block(d5, e2, 128, dropout=False)\n",
        "  d7 = decoder_block(d6, e1, 64, dropout=False)\n",
        "  g = Conv2DTranspose(3, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d7)\n",
        "  out_image = Activation('tanh')(g)\n",
        "  model = Model(in_image, out_image)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkbqRmxPA27y"
      },
      "outputs": [],
      "source": [
        "def define_gan(g_model, d_model, image_shape):\n",
        "\n",
        "\td_model.trainable = False\n",
        "\n",
        "\tin_src = Input(shape=image_shape)\n",
        "\n",
        "\tgen_out = g_model(in_src)\n",
        "\t\n",
        "\tdis_out = d_model([in_src, gen_out])\n",
        "\n",
        "\tmodel = Model(in_src, [dis_out, gen_out])\n",
        "\n",
        "\topt = Adam(lr=0.0002, beta_1=0.5)\n",
        "\tmodel.compile(loss=['binary_crossentropy', 'mae'], optimizer=opt, loss_weights=[1,100])\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HHvxUsZA50D"
      },
      "outputs": [],
      "source": [
        "def load_real_samples(filename):\n",
        "\n",
        "\tdata = load(filename)\n",
        "\n",
        "\tX1, X2 = data['arr_0'], data['arr_1']\n",
        "\n",
        "\tX1 = (X1 - 127.5) / 127.5\n",
        "\tX2 = (X2 - 127.5) / 127.5\n",
        "\treturn [X1, X2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZ_opOeJA9E7"
      },
      "outputs": [],
      "source": [
        "from numpy.random import randint\n",
        "from numpy import ones\n",
        "def generate_real_samples(dataset, n_samples, patch_shape):\n",
        "\n",
        "\ttrainA, trainB = dataset\n",
        "\n",
        "\tix = randint(0, trainA.shape[0], n_samples)\n",
        "\n",
        "\tX1, X2 = trainA[ix], trainB[ix]\n",
        "\n",
        "\ty = ones((n_samples, patch_shape, patch_shape, 1))\n",
        "\treturn [X1, X2], y"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RzwDRxqRM24G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa1CrVp7A_iQ"
      },
      "outputs": [],
      "source": [
        "from numpy import zeros\n",
        "def generate_fake_samples(g_model, samples, patch_shape):\n",
        "\n",
        "\tX = g_model.predict(samples)\n",
        "\n",
        "\ty = zeros((len(X), patch_shape, patch_shape, 1))\n",
        "\treturn X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xSO2hGfBBTv"
      },
      "outputs": [],
      "source": [
        "def summarize_performance(step, g_model, dataset, n_samples=3):\n",
        "\n",
        "  [X_realA, X_realB], _ = generate_real_samples(dataset, n_samples, 1)\n",
        "\n",
        "  X_fakeB, _ = generate_fake_samples(g_model, X_realA, 1)\n",
        "\n",
        "  X_realA = (X_realA + 1) / 2.0\n",
        "  X_realB = (X_realB + 1) / 2.0\n",
        "  X_fakeB = (X_fakeB + 1) / 2.0\n",
        "\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_realA[i])\n",
        "\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + n_samples + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_fakeB[i])\n",
        "\n",
        "  for i in range(n_samples):\n",
        "    pyplot.subplot(3, n_samples, 1 + n_samples*2 + i)\n",
        "    pyplot.axis('off')\n",
        "    pyplot.imshow(X_realB[i])\n",
        "\t# save plot to file\n",
        "  filename1 = 'drive/My Drive/maps/plot/plot2_%06d.png' % (step+1)\n",
        "  pyplot.savefig(filename1)\n",
        "  pyplot.close()\n",
        "  # save the generator model\n",
        "  filename2 = 'drive/My Drive/maps/models/model2_%06d.h5' % (step+1)\n",
        "  g_model.save(filename2)\n",
        "  print('>Saved: %s and %s' % (filename1, filename2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baFK_fLRBIy9"
      },
      "outputs": [],
      "source": [
        "# create a line plot of loss for the gan and save to file\n",
        "def plot_history(d1_hist, d2_hist, g_hist, a1_hist, a2_hist):\n",
        "\t# plot loss\n",
        "\tpyplot.subplot(2, 1, 1)\n",
        "\tpyplot.plot(d1_hist, label='d-real')\n",
        "\tpyplot.plot(d2_hist, label='d-fake')\n",
        "\tpyplot.plot(g_hist, label='gen')\n",
        "\tpyplot.legend()\n",
        "\t# plot discriminator accuracy\n",
        "\tpyplot.subplot(2, 1, 2)\n",
        "\tpyplot.plot(a1_hist, label='acc-real')\n",
        "\tpyplot.plot(a2_hist, label='acc-fake')\n",
        "\tpyplot.legend()\n",
        "\t# save plot to file\n",
        "\tpyplot.savefig('drive/My Drive/maps/plot_line_plot_loss.png')\n",
        "\tpyplot.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFOKw5AdBM4v"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "arr=[]\n",
        "\n",
        "def train(d_model, g_model, gan_model, dataset, n_epochs=100, n_batch=1):\n",
        "\n",
        "  n_patch = d_model.output_shape[1]\n",
        "  trainA, trainB = dataset\n",
        "  bat_per_epo = int(len(trainA) / n_batch)\n",
        "  n_steps = bat_per_epo * n_epochs\n",
        "  d1_hist, d2_hist, g_hist, a1_hist, a2_hist = list(), list(), list(), list(), list()\n",
        "\n",
        "  for i in range(n_steps):\n",
        "    \n",
        "    [X_realA, X_realB], y_real = generate_real_samples(dataset, n_batch, n_patch)\n",
        "    X_fakeB, y_fake = generate_fake_samples(g_model, X_realA, n_patch)\n",
        "    d_loss1 = d_model.train_on_batch([X_realA, X_realB], y_real)\n",
        "    d_loss2 = d_model.train_on_batch([X_realA, X_fakeB], y_fake)\n",
        "    _,_,g_loss = gan_model.train_on_batch(X_realA, [y_real, X_realB])\n",
        "    d1_hist.append(d_loss1)\n",
        "    d2_hist.append(d_loss2)\n",
        "    g_hist.append(g_loss)\n",
        "   \n",
        "\t\t\n",
        "    if i%100 == 0:\n",
        "      \n",
        "      print('>%d, d1[%.3f] d2[%.3f] g[%.3f]' % (i+1, d_loss1, d_loss2, g_loss))\n",
        "      \n",
        "\t\t "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fe-olioNBROm",
        "outputId": "e2e5e84f-4bcb-4831-c20d-5a7a10fc987e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded (1096, 256, 256, 3) (1096, 256, 256, 3)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">1, d1[0.650] d2[0.543] g[0.755]\n",
            ">101, d1[0.031] d2[0.073] g[0.116]\n",
            ">201, d1[0.016] d2[0.007] g[0.090]\n",
            ">301, d1[0.123] d2[0.022] g[0.071]\n",
            ">401, d1[0.006] d2[0.022] g[0.079]\n",
            ">501, d1[0.004] d2[0.009] g[0.195]\n",
            ">601, d1[0.007] d2[0.005] g[0.086]\n",
            ">701, d1[0.031] d2[0.003] g[0.096]\n",
            ">801, d1[0.447] d2[0.038] g[0.079]\n",
            ">901, d1[0.081] d2[0.004] g[0.104]\n",
            ">1001, d1[0.007] d2[0.003] g[0.113]\n",
            ">1101, d1[0.145] d2[0.792] g[0.083]\n"
          ]
        }
      ],
      "source": [
        "from numpy import load\n",
        "dataset = load_real_samples('maps_256.npz')\n",
        "print('Loaded', dataset[0].shape, dataset[1].shape)\n",
        "image_shape = dataset[0].shape[1:]\n",
        "d_model = define_discriminator(image_shape)\n",
        "g_model = define_generator(image_shape)\n",
        "gan_model = define_gan(g_model, d_model, image_shape)\n",
        "train(d_model, g_model, gan_model, dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8gmbI5WEBUOB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}